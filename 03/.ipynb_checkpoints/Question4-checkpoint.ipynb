{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import errno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets.mnist\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2049\n",
    "    length = get_int(data[4:8])\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "    return torch.from_numpy(parsed).view(length).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2051\n",
    "    length = get_int(data[4:8])\n",
    "    num_rows = get_int(data[8:12])\n",
    "    num_cols = get_int(data[12:16])\n",
    "    images = []\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "    return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTPairs(torch.utils.data.Dataset):\n",
    "    raw_folder = 'MNIST/raw'\n",
    "    processed_folder = 'MNIST/processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train # training set or test set\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "\n",
    "            train_labels_class = []\n",
    "            train_data_class = []\n",
    "            for i in range(10):\n",
    "                indices = torch.squeeze((self.train_labels == i).nonzero())\n",
    "                train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
    "                train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
    "\n",
    "            # generate balanced pairs\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            lengths = [x.shape[0] for x in train_labels_class]\n",
    "            for i in range(10):\n",
    "                for j in range(500): # create 500 pairs\n",
    "                    rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "                    if rnd_cls >= i:\n",
    "                        rnd_cls = rnd_cls + 1\n",
    "\n",
    "                    rnd_dist = random.randint(0, 100)\n",
    "\n",
    "                    self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
    "                    self.train_labels.append([1,0])\n",
    "\n",
    "            self.train_data = torch.stack(self.train_data)\n",
    "            self.train_labels = torch.tensor(self.train_labels)\n",
    "\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "            test_labels_class = []\n",
    "            test_data_class = []\n",
    "            for i in range(10):\n",
    "                indices = torch.squeeze((self.test_labels == i).nonzero())\n",
    "                test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
    "                test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
    "\n",
    "            # generate balanced pairs\n",
    "            self.test_data = []\n",
    "            self.test_labels = []\n",
    "            lengths = [x.shape[0] for x in test_labels_class]\n",
    "            for i in range(10):\n",
    "                for j in range(500): # create 500 pairs\n",
    "                    rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "                    if rnd_cls >= i:\n",
    "                        rnd_cls = rnd_cls + 1\n",
    "\n",
    "                    rnd_dist = random.randint(0, 100)\n",
    "\n",
    "                    self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
    "                    self.test_labels.append([1,0])\n",
    "\n",
    "            self.test_data = torch.stack(self.test_data)\n",
    "            self.test_labels = torch.tensor(self.test_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            imgs, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            imgs, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        img_ar = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            img_ar.append(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img_ar, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "        self.linear1 = nn.Linear(2304, 512)\n",
    "\n",
    "        self.linear2 = nn.Linear(512, 2)\n",
    "    def get_embedding(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    def forward_from_embedding(self, emb1, emb2):\n",
    "        res = torch.abs(emb2 - emb1)\n",
    "        res = self.linear2(res)\n",
    "        return res\n",
    "\n",
    "    def forward(self, data):\n",
    "        res = []\n",
    "        for i in range(2): # Siamese nets; sharing weights\n",
    "            x = data[i]\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv3(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = self.linear1(x)\n",
    "            res.append(F.relu(x))\n",
    "\n",
    "        res = torch.abs(res[1] - res[0])\n",
    "        res = self.linear2(res)\n",
    "        return res\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward_dist_label(self, dist, label):\n",
    "        loss = torch.mean(1/2*(label) * torch.pow(dist, 2) +\n",
    "                                      1/2*(1-label) * torch.pow(torch.clamp(self.margin - dist, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "        dist1 = torch.norm(anchor-positive,dim=1)\n",
    "#         dist1 = (anchor - positive).pow(2).sum(1)\n",
    "        label1 = t_p\n",
    "        \n",
    "        dist2 = torch.norm(anchor-negative,dim=1)\n",
    "#         dist2 = (anchor - negative).pow(2).sum(1)\n",
    "        label2 = t_n\n",
    "        \n",
    "        loss = self.forward_dist_label(dist1,label1) + self.forward_dist_label(dist2,label2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "\n",
    "#         squarred_distance_1 = torch.norm(anchor-positive,dim=1)\n",
    "        \n",
    "#         squarred_distance_2 = torch.norm(anchor-negative,dim=1)\n",
    "\n",
    "        squarred_distance_1 = (anchor - positive).pow(2).sum(1)\n",
    "        \n",
    "        squarred_distance_2 = (anchor - negative).pow(2).sum(1)\n",
    "        \n",
    "        triplet_loss = F.relu( self.margin + squarred_distance_1 - squarred_distance_2 ).mean()\n",
    "        \n",
    "        return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Cross Entropy loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CrossEntLoss, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "\n",
    "        loss_positive = F.cross_entropy(op_p, t_p)\n",
    "        loss_negative = F.cross_entropy(op_n, t_n)\n",
    "        loss = loss_positive + loss_negative\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    curr_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embedding_anchor = model.get_embedding(data[0])\n",
    "        embedding_positive = model.get_embedding(data[1])\n",
    "        embedding_negative = model.get_embedding(data[2])\n",
    "        \n",
    "        output_positive = model.forward_from_embedding(embedding_anchor, embedding_positive)\n",
    "        output_negative = model.forward_from_embedding(embedding_anchor, embedding_negative)\n",
    "#         output_positive = model([data[0], data[1]])\n",
    "#         output_negative = model([data[0], data[2]])\n",
    "\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        target_positive = torch.squeeze(target[:,0])\n",
    "        target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "#         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "#         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "#         loss = loss_positive + loss_negative\n",
    "        \n",
    "        loss = loss_fn(embedding_anchor,embedding_positive,embedding_negative,\n",
    "                       output_positive,output_negative, target_positive, target_negative)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        curr_loss += loss.item()\n",
    "        \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1)*batch_size, len(train_loader.dataset), 100. * (batch_idx+1)*batch_size / len(train_loader.dataset),\n",
    "                curr_loss*1.0/((batch_idx+1)*batch_size)),end='\\t\\t\\t')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            \n",
    "            embedding_anchor = model.get_embedding(data[0])\n",
    "            embedding_positive = model.get_embedding(data[1])\n",
    "            embedding_negative = model.get_embedding(data[2])\n",
    "\n",
    "            output_positive = model.forward_from_embedding(embedding_anchor, embedding_positive)\n",
    "            output_negative = model.forward_from_embedding(embedding_anchor, embedding_negative)\n",
    "#             output_positive = model([data[0], data[1]])\n",
    "#             output_negative = model([data[0], data[2]])\n",
    "\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            target_positive = torch.squeeze(target[:,0])\n",
    "            target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "#             loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "#             loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "\n",
    "#             loss += loss_positive + loss_negative\n",
    "\n",
    "            loss += loss_fn(embedding_anchor,embedding_positive,embedding_negative,\n",
    "                           output_positive, output_negative, target_positive, target_negative)\n",
    "    \n",
    "            accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "            accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "\n",
    "            accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "            all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "\n",
    "        accuracy = 100. * accurate_labels / all_labels\n",
    "        print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss/all_labels))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "#         out1 = model.get_embedding(data[0])\n",
    "#         out2 = model.get_embedding(data[1])\n",
    "        output = model([data[0],data[1]])\n",
    "        return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=False, transform=trans), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5000/5000 (100%)]\tLoss: nan\t\t\t\t\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f7cbaac7c8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-e401366ccd99>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, loss_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-adca79e68b61>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mimg_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# embedding_net = EmbeddingNet().to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = ContrastiveLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, epoch, optimizer,loss_fn)\n",
    "    test(model, device, test_loader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5000/5000 (100%)]\tLoss: 0.031026\t\t\t\n",
      "Test accuracy: 5958/10000 (59.580%)\tLoss: 0.011032\n",
      "Train Epoch: 1 [5000/5000 (100%)]\tLoss: 0.015963\t\t\t\n",
      "Test accuracy: 5310/10000 (53.100%)\tLoss: 0.006869\n",
      "Train Epoch: 2 [5000/5000 (100%)]\tLoss: 0.011391\t\t\t\n",
      "Test accuracy: 5930/10000 (59.300%)\tLoss: 0.007760\n",
      "Train Epoch: 3 [5000/5000 (100%)]\tLoss: 0.010135\t\t\t\n",
      "Test accuracy: 5917/10000 (59.170%)\tLoss: 0.005919\n",
      "Train Epoch: 4 [5000/5000 (100%)]\tLoss: 0.005066\t\t\t\n",
      "Test accuracy: 5957/10000 (59.570%)\tLoss: 0.006271\n",
      "Train Epoch: 5 [5000/5000 (100%)]\tLoss: 0.006110\t\t\t\n"
     ]
    }
   ],
   "source": [
    "# embedding_net = EmbeddingNet().to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = TripletLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, epoch, optimizer,loss_fn)\n",
    "    test(model, device, test_loader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5000/5000 (100%)]\tLoss: 0.047354\t\t\t\n",
      "Test accuracy: 8460/10000 (84.600%)\tLoss: 0.018568\n",
      "Train Epoch: 1 [5000/5000 (100%)]\tLoss: 0.024589\t\t\t\n",
      "Test accuracy: 9023/10000 (90.230%)\tLoss: 0.013119\n",
      "Train Epoch: 2 [5000/5000 (100%)]\tLoss: 0.014913\t\t\t\n",
      "Test accuracy: 9253/10000 (92.530%)\tLoss: 0.010105\n",
      "Train Epoch: 3 [5000/5000 (100%)]\tLoss: 0.008847\t\t\t\n",
      "Test accuracy: 9313/10000 (93.130%)\tLoss: 0.010797\n",
      "Train Epoch: 4 [5000/5000 (100%)]\tLoss: 0.006702\t\t\t\n",
      "Test accuracy: 9408/10000 (94.080%)\tLoss: 0.009446\n",
      "Train Epoch: 5 [5000/5000 (100%)]\tLoss: 0.004167\t\t\t\n",
      "Test accuracy: 9523/10000 (95.230%)\tLoss: 0.009084\n",
      "Train Epoch: 6 [5000/5000 (100%)]\tLoss: 0.003367\t\t\t\n",
      "Test accuracy: 9396/10000 (93.960%)\tLoss: 0.010587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8c5bb7ee258d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-97a01b91785f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, epoch, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-adca79e68b61>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mimg_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# embedding_net = EmbeddingNet().to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = CrossEntLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, device, train_loader, epoch, optimizer,loss_fn)\n",
    "    test(model, device, test_loader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAENRJREFUeJzt3XuQlfV9x/HPF1gg3LyAQbKsd6pSTdCueEvVFMwQ2oqXNtWxCaZOMRXrJTaVOm3iOJNKjLc0RisUBKcqmvGKdVqRmlKCWheiXFxug5BAFxANuNGI7O63f+yxs+H3W/bsnuecs8+P92uG2XM++zvn+T27X748nOdm7i4AQP71qfYEAADZoKEDQCJo6ACQCBo6ACSChg4AiaChA0AiaOgAkAgaOgAkoqSGbmaTzGydmW00sxlZTQqoNmobeWQ9PVPUzPpKWi/pQklbJb0h6Qp3fzu76QGVR20jr/qV8Nrxkja6+yZJMrMFkqZI6rTo+9sAH6jBJSwS6NzH+lCf+F7L4K2obfQqxdZ2KQ29VtIvOzzfKunMA71goAbrTJtQwiKBzr3ui7N6K2obvUqxtV1KQy+KmU2TNE2SBmpQuRcHVAy1jd6mlJ2i2yTVdXg+upD9Fnef5e717l5fowElLA6oGGobuVRKQ39D0hgzO9bM+ku6XNLz2UwLqCpqG7nU449c3L3FzK6T9B+S+kqa6+5rMpsZUCXUNvKqpM/Q3f1FSS9mNBeg16C2kUecKQoAiaChA0AiaOgAkAgaOgAkgoYOAImgoQNAImjoAJAIGjoAJIKGDgCJoKEDQCJo6ACQCBo6ACSChg4AiaChA0Aiyn4LOgAoxZ4rz4rmzZc2B9ntpy6Mjv3HdZOC7JAfDYuOrXmpoRuz613YQgeARNDQASARNHQASAQNHQASUdJOUTPbLKlZUqukFnevz2JSQLVR29Wx6c6zg2zJ5T+Ijn2i+ZQgm/HMldGxY8ZvCbLbHno0OvY7f/qNIPOG1dGxvU0WR7l8yd13ZfA+QG9DbSNX+MgFABJRakN3SS+Z2XIzm5bFhIBegtpG7pT6kcsX3X2bmX1W0iIzW+vuSzoOKPxlmCZJAzWoxMUBFUNtI3dK2kJ3922FrzslPSNpfGTMLHevd/f6Gg0oZXFAxVDbyKMeb6Gb2WBJfdy9ufD4y5Juz2xmQJVQ2+XnZ38hmq++8p+C7Pd+9LfRsbUzlwXZcXo1vsBDDwmi25+9KDp0/fX9g2zM1+Nv29uU8pHLSEnPmNmn7/OYu/97JrMCqovaRi71uKG7+yZJ8X9mgRyjtpFXHLYIAImgoQNAIrgeejf1HTE8mr+4cnGQXbstfh3nzZPCQ9xa33s/yHZdE54GLUnNRx9ohl0b8ZZH86FPvFbaGwMRbeefFmS3PTwnOvbC6dODrPbZcOdnd7Xu3hNk5xy+Izp2zfrRJS+vWthCB4BE0NABIBE0dABIBA0dABJBQweARHCUSzd9cMEJ0bzVFwXZyy+He/clqeXevUH2k98PT0Q8oSZ+GvMQK+26Ie+1/SaaTx72N0E2fHYnp1IDEa1fOj3Irp+1IMiueei66OuzOKIlpulb5wTZNw+L3zhj0cLzyjKHSmALHQASQUMHgETQ0AEgETR0AEgEO0UPoO/JY4Lsobvu62R0uKOy8es/7sbSwl/FTU3nRkc27jkyyP6ybklkpHTZ4F8F2fA+n4mOHfqLfQeaIPD/+gyK36Fp9B3rg+zezRcGWe33y7PzU+NPjcZ3XDs3yL65eUp07MCF/5PplCqJLXQASAQNHQASQUMHgETQ0AEgEV02dDOba2Y7zWx1h+xwM1tkZhsKXw8r7zSB7FHbSE0xR7nMk3S/pEc6ZDMkLXb3mWY2o/D8luynV12NNx0aZCfVlHbavSSNXXpVkB35rwODbMgbW6Kv77f73SC766kvR8deNu6JIJuw5tLo2M8sbQyytujIZMzTQVrbpdr55/Fbrv7RsBeCrGlS3yBrzWAOVn9KkH378ceiY+dsD0/n/+iS+I1e8qzLLXR3XyJp/9vpTJE0v/B4vqSLM54XUHbUNlLT08/QR7p7U+HxdkkjM5oPUG3UNnKr5J2i7u6SOv2/i5lNM7MGM2vYp/Aqg0BvRW0jb3ra0HeY2ShJKnzd2dlAd5/l7vXuXl8TOZsS6GWobeRWT0/9f17SVEkzC1+fy2xGvcgRyyI/nj+Mj131SXja/NV33Bgde9zjq4Osrbk5yFo6mdem758dZGvHFX+Zga3vxg/cOP7DzUW/R8IOitoul8YPPxdkrbv3lPSeH116ZjT/hzvD0/lnbz8/Ova9W44Ksj673ixpXr1RMYctPi7pVUknmtlWM7ta7cV+oZltkDSx8BzIFWobqelyC93dr+jkWxMyngtQUdQ2UsOZogCQCBo6ACSChg4AieAGFwcw/Oe7ix776K/OCrIjHl4eHdu275Mez0mShp68/8mN3TPy2f4lvR7ozMRD1wTZsJ8fF2QLVtYX/Z4rJ/wwmt+wdWKQvf/tuujYPsvSO6Ilhi10AEgEDR0AEkFDB4BE0NABIBHsFM3IzJHhDtBzrpgeHXvoI68GWZ9xY4Ns3V8Mjb7+uuMWFT2vqVv+IMiG/duq6NjEr32OCvjjQR8E2U2LwwMGJp4W7jyVpGVbjwmyM2Z/Kzr22Ac2BJm9+1YXM0wbW+gAkAgaOgAkgoYOAImgoQNAItgpegBtK9cG2clLvhEd23jew0E29trwuueStGNheD3ydVcNC7INlz3Q1RS7tOyNk4JszEevlfy+OHjYgPDmHbvHxm/k1NfCbcShR4bX+v/fKUOirx+9Pb6zNCaLG02nhi10AEgEDR0AEkFDB4BE0NABIBHF3FN0rpntNLPVHbLbzGybmb1Z+DO5vNMEskdtIzXFHOUyT9L9kh7ZL7/X3e/KfEa9iYd78sfcGr9G+g8XnhBk/1L3X9GxP1se/jv6hf4vRkaGRxd0pqWTff6Hr7ai3+MgNE8Ha21H9Ds6fi3xd+4+JMheOiP+41nzSd8ge2LcnCC7eXj8aDFt33GAGaIrXW6hu/sSSaXdUQHohahtpKaUz9CvM7OVhf+2hgdWA/lFbSOXetrQH5R0vKRxkpok3d3ZQDObZmYNZtawT3t7uDigYqht5FaPGrq773D3VndvkzRb0vgDjJ3l7vXuXl/Tjc+EgWqgtpFnPTr138xGuXtT4eklkuLnuCeo5Z0t0XzxRZ8Psv+ce2J07Pkj1gfZuQPCazt3R3Nb/MbTw2eH115H5w6W2rZ+4V/91ofjV8T/yiFvB9n0S6+Jjl1/ffgP233nLAiXtWZdV1NED3TZ0M3scUkXSBphZlslfVfSBWY2TpJL2iwp/tsFejFqG6npsqG7+xWRODwOCcgZahup4UxRAEgEDR0AEkFDB4BEcIOLjLRs2hyGF8TH/vTYU4Ls2fqJQTbx75dGX/+dEau6MTMgtO7+04Ns44n/HB179q3Tg+yw5fGjp45acEaQHXneniDrN7o2+vqWrduiOYrDFjoAJIKGDgCJoKEDQCJo6ACQCHaKVkHs8gGDPxde1G/C0OLvgA7E9Bk4MJr/3fkvBNnv/mxqdOwxjy0PsvBOAe0GbdgVZG0ebjfuOXN09PWD2SlaErbQASARNHQASAQNHQASQUMHgETQ0AEgERzl0kt8WBsejXDugPgNB2Imr7wqmh+m0m6cgXyz/v2j+dXDtgbZPY1Do2N9X/zmKTGtG98JsjV7w9P8t02IHyfzO08VvShEsIUOAImgoQNAImjoAJCILhu6mdWZ2Stm9raZrTGzGwr54Wa2yMw2FL6GpzoCvRi1jdQUs1O0RdLN7r7CzIZKWm5miyRdJWmxu880sxmSZki6pXxTxYH8+rUjojk7RQ+I2u5g7+jid352avypQXTh4PC6/t9rsdKXhUCXW+ju3uTuKwqPmyU1SqqVNEXS/MKw+ZIuLtckgXKgtpGabn2GbmbHSDpN0uuSRrp7U+Fb2yWNzHRmQAVR20hB0Q3dzIZIekrSje7+Qcfvuburkwuwmdk0M2sws4Z92lvSZIFyoLaRiqIaupnVqL3gH3X3pwvxDjMbVfj+KEk7Y69191nuXu/u9TUakMWcgcxQ20hJMUe5mKQ5khrd/Z4O33pe0qcXUJ4q6bnspweUD7WN1BRzlMu5kr4maZWZvVnIbpU0U9KTZna1pC2SvlqeKWJ/i38Tbg0e/YMV0bHFXzzgoJR8bbc2N0fz81b9SZCtnfRgdOzYe/46yE66MzzFX5LePXlIkNX2HRRko5ZylEs5dNnQ3X2ppM5++hOynQ5QOdQ2UsOZogCQCBo6ACSChg4AieB66DnUFvl3uO3jj6swE/R6Hr/u+IC7wsvTvDU7/hbr/+yBIFt8Ufwwzbp+zwTZTz8Or7M+bO3u6OvZiV8attABIBE0dABIBA0dABJBQweARNDQASARHOWSQz/ZdUYk/XXF54H8qnl5eZDNuOavomO3TwuPoKqv/UV0bMMLpwTZ0QvfD7K2lWu7miJ6gC10AEgEDR0AEkFDB4BE0NABIBHsFM2hFY98Psg+q2VVmAlSUvNSQzSveynMdnTyHnWROuR0/sphCx0AEkFDB4BE0NABIBHF3CS6zsxeMbO3zWyNmd1QyG8zs21m9mbhz+TyTxfIDrWN1BSzU7RF0s3uvsLMhkpabmaLCt+7193vKt/0gLKitpGUYm4S3SSpqfC42cwaJdWWe2JAuVHbSE23PkM3s2MknSbp9UJ0nZmtNLO5ZhbeAgXICWobKSi6oZvZEElPSbrR3T+Q9KCk4yWNU/tWzt2dvG6amTWYWcM+7c1gykC2qG2koqiGbmY1ai/4R939aUly9x3u3urubZJmSxofe627z3L3enevr1H8PoRAtVDbSEkxR7mYpDmSGt39ng75qA7DLpG0OvvpAeVDbSM1xRzlcq6kr0laZWZvFrJbJV1hZuMkuaTNkq4pywyB8qG2kZRijnJZKski33ox++kAlUNtIzWcKQoAiaChA0AiaOgAkAgaOgAkghtc9BJDnnwtyCY/eXp0LDezABDDFjoAJIKGDgCJoKEDQCJo6ACQCHP3yi3M7F1JWwpPR0jaVbGFVw7rVT1Hu/sR1Vhwh9rOw8+pp1JdtzysV1G1XdGG/lsLNmtw9/qqLLyMWK+DW8o/p1TXLaX14iMXAEgEDR0AElHNhj6rissuJ9br4JbyzynVdUtmvar2GToAIFt85AIAiah4QzezSWa2zsw2mtmMSi8/S4U7wu80s9UdssPNbJGZbSh8zd0d482szsxeMbO3zWyNmd1QyHO/buWUSm1T1/lbt09VtKGbWV9JP5b0FUlj1X6rr7GVnEPG5kmatF82Q9Jidx8jaXHhed60SLrZ3cdKOkvS9MLvKYV1K4vEanueqOtcqvQW+nhJG919k7t/ImmBpCkVnkNm3H2JpPf3i6dIml94PF/SxRWdVAbcvcndVxQeN0tqlFSrBNatjJKpbeo6f+v2qUo39FpJv+zwfGshS8lId28qPN4uaWQ1J1MqMztG0mmSXldi65ax1Gs7qd99qnXNTtEy8vZDiHJ7GJGZDZH0lKQb3f2Djt/L+7qh5/L+u0+5rivd0LdJquvwfHQhS8kOMxslSYWvO6s8nx4xsxq1F/2j7v50IU5i3cok9dpO4nefel1XuqG/IWmMmR1rZv0lXS7p+QrPodyelzS18HiqpOeqOJceMTOTNEdSo7vf0+FbuV+3Mkq9tnP/uz8Y6rriJxaZ2WRJ90nqK2muu3+vohPIkJk9LukCtV+tbYek70p6VtKTko5S+9X3vuru++9g6tXM7IuS/lvSKklthfhWtX/emOt1K6dUapu6zt+6fYozRQEgEewUBYBE0NABIBE0dABIBA0dABJBQweARNDQASARNHQASAQNHQAS8X9Z/gOAN1lGtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two images are not of the same number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two images are of the same number\n"
     ]
    }
   ],
   "source": [
    "prediction_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=False, transform=trans), batch_size=1, shuffle=True)\n",
    "\n",
    "triplets = next(iter(prediction_loader))\n",
    "\n",
    "data = []\n",
    "data.extend(triplets[0][:3:2])\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(data[0].numpy()[0,0])\n",
    "axes[1].imshow(data[1].numpy()[0,0])\n",
    "plt.show()\n",
    "same = oneshot(model, device, data)\n",
    "if same > 0:\n",
    "    print('These two images are of the same number')\n",
    "else:\n",
    "    print('These two images are not of the same number')\n",
    "    \n",
    "data = []\n",
    "data.extend(triplets[0][:2])\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(data[0].numpy()[0,0])\n",
    "axes[1].imshow(data[1].numpy()[0,0])\n",
    "plt.show()\n",
    "same = oneshot(model, device, data)\n",
    "if same > 0:\n",
    "    print('These two images are of the same number')\n",
    "else:\n",
    "    print('These two images are not of the same number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
