{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import errno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets.mnist\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2049\n",
    "    length = get_int(data[4:8])\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "    return torch.from_numpy(parsed).view(length).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2051\n",
    "    length = get_int(data[4:8])\n",
    "    num_rows = get_int(data[8:12])\n",
    "    num_cols = get_int(data[12:16])\n",
    "    images = []\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "    return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTPairs(torch.utils.data.Dataset):\n",
    "    raw_folder = 'MNIST/raw'\n",
    "    processed_folder = 'MNIST/processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train # training set or test set\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "\n",
    "            train_labels_class = []\n",
    "            train_data_class = []\n",
    "            for i in range(10):\n",
    "                indices = torch.squeeze((self.train_labels == i).nonzero())\n",
    "                train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
    "                train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
    "\n",
    "            # generate balanced pairs\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            lengths = [x.shape[0] for x in train_labels_class]\n",
    "            for i in range(10):\n",
    "                for j in range(500): # create 500 pairs\n",
    "                    rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "                    if rnd_cls >= i:\n",
    "                        rnd_cls = rnd_cls + 1\n",
    "\n",
    "                    rnd_dist = random.randint(0, 100)\n",
    "\n",
    "                    self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
    "                    self.train_labels.append([1,0])\n",
    "\n",
    "            self.train_data = torch.stack(self.train_data)\n",
    "            self.train_labels = torch.tensor(self.train_labels)\n",
    "\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "            test_labels_class = []\n",
    "            test_data_class = []\n",
    "            for i in range(10):\n",
    "                indices = torch.squeeze((self.test_labels == i).nonzero())\n",
    "                test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
    "                test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
    "\n",
    "            # generate balanced pairs\n",
    "            self.test_data = []\n",
    "            self.test_labels = []\n",
    "            lengths = [x.shape[0] for x in test_labels_class]\n",
    "            for i in range(10):\n",
    "                for j in range(500): # create 500 pairs\n",
    "                    rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "                    if rnd_cls >= i:\n",
    "                        rnd_cls = rnd_cls + 1\n",
    "\n",
    "                    rnd_dist = random.randint(0, 100)\n",
    "\n",
    "                    self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
    "                    self.test_labels.append([1,0])\n",
    "\n",
    "            self.test_data = torch.stack(self.test_data)\n",
    "            self.test_labels = torch.tensor(self.test_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            imgs, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            imgs, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        img_ar = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            img_ar.append(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img_ar, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "        self.linear1 = nn.Linear(2304, 512)\n",
    "\n",
    "        self.linear2 = nn.Linear(512, 2)\n",
    "    def get_embedding(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, data):\n",
    "        res = []\n",
    "        for i in range(2): # Siamese nets; sharing weights\n",
    "            x = data[i]\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv3(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = self.linear1(x)\n",
    "            res.append(F.relu(x))\n",
    "\n",
    "        res = torch.abs(res[1] - res[0])\n",
    "        res = self.linear2(res)\n",
    "        return res\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "        self.linear1 = nn.Linear(2304, 512)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,embedding_net):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.linear1 = nn.Linear(512, 4)\n",
    "        self.linear2 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        x1 = self.embedding_net(input1)\n",
    "        x2 = self.embedding_net(input2)\n",
    "        x = x1-x2\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "    def forward_from_embedding(self, out1, out2):\n",
    "        x = out1-out2\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,embedding_net):\n",
    "        super().__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.linear1 = nn.Linear(512, 4)\n",
    "        self.linear2 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        x1 = self.embedding_net(input1)\n",
    "        x2 = self.embedding_net(input2)\n",
    "        x = x1-x2\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "    def forward_from_embedding(self, out1, out2):\n",
    "        x = out1-out2\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward_dist_label(self, dist, label):\n",
    "        loss = torch.mean(1/2*(label) * torch.pow(dist, 2) +\n",
    "                                      1/2*(1-label) * torch.pow(torch.clamp(self.margin - dist, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "        dist1 = torch.norm(anchor-positive,dim=1)\n",
    "        label1 = t_p\n",
    "        \n",
    "        dist2 = torch.norm(anchor-negative,dim=1)\n",
    "        label2 = t_n\n",
    "        \n",
    "        loss = self.forward_dist_label(dist1,label1) + self.forward_dist_label(dist2,label2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "\n",
    "        squarred_distance_1 = (anchor - positive).pow(2).sum(1)\n",
    "        \n",
    "        squarred_distance_2 = (anchor - negative).pow(2).sum(1)\n",
    "        \n",
    "        triplet_loss = F.relu( self.margin + squarred_distance_1 - squarred_distance_2 ).mean()\n",
    "        \n",
    "        return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CrossEntLoss, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, positive, negative, op_p, op_n, t_p, t_n):\n",
    "\n",
    "        loss_positive = F.cross_entropy(op_p, t_p)\n",
    "        loss_negative = F.cross_entropy(op_n, t_n)\n",
    "        loss = loss_positive + loss_negative\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    curr_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embedding_anchor = embedding_net(data[0])\n",
    "        embedding_positive = embedding_net(data[1])\n",
    "        embedding_negative = embedding_net(data[2])\n",
    "        \n",
    "        output_positive = model.forward_from_embedding(embedding_anchor, embedding_positive)\n",
    "        output_negative = model.forward_from_embedding(embedding_anchor, embedding_negative)\n",
    "\n",
    "        target = target.type(torch.LongTensor).to(device)\n",
    "        target_positive = torch.squeeze(target[:,0])\n",
    "        target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "#         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "#         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "#         loss = loss_positive + loss_negative\n",
    "        \n",
    "        loss = loss_fn(embedding_anchor,embedding_positive,embedding_negative,\n",
    "                       output_positive,output_negative, target_positive, target_negative)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        curr_loss += loss.item()\n",
    "        \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1)*batch_size, len(train_loader.dataset), 100. * (batch_idx+1)*batch_size / len(train_loader.dataset),\n",
    "                curr_loss*1.0/((batch_idx+1)*batch_size)),end='\\t\\t\\t')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(embedding_net, model, device, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].to(device)\n",
    "            \n",
    "            embedding_anchor = embedding_net(data[0])\n",
    "            embedding_positive = embedding_net(data[1])\n",
    "            embedding_negative = embedding_net(data[2])\n",
    "\n",
    "            output_positive = model.forward_from_embedding(embedding_anchor, embedding_positive)\n",
    "            output_negative = model.forward_from_embedding(embedding_anchor, embedding_negative)\n",
    "\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            target_positive = torch.squeeze(target[:,0])\n",
    "            target_negative = torch.squeeze(target[:,1])\n",
    "\n",
    "#             loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "#             loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "\n",
    "#             loss += loss_positive + loss_negative\n",
    "\n",
    "            loss += loss_fn(embedding_anchor,embedding_positive,embedding_negative,\n",
    "                           output_positive, output_negative, target_positive, target_negative)\n",
    "    \n",
    "            accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "            accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "\n",
    "            accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "            all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "\n",
    "        accuracy = 100. * accurate_labels / all_labels\n",
    "        print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss/all_labels))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(embedding_net, model, device, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "        out1 = embedding_net(data[0])\n",
    "        out2 = embedding_net(data[1])\n",
    "        output = model.forward_from_embedding(out1,out2)\n",
    "        return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=False, transform=trans), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5000/5000 (100%)]\tLoss: 0.034595\t\t\t\n",
      "Test accuracy: 5053/10000 (50.530%)\tLoss: 0.012786\n",
      "Train Epoch: 1 [5000/5000 (100%)]\tLoss: 0.016144\t\t\t\n",
      "Test accuracy: 5034/10000 (50.340%)\tLoss: 0.009958\n",
      "Train Epoch: 2 [5000/5000 (100%)]\tLoss: 0.009757\t\t\t\n",
      "Test accuracy: 5009/10000 (50.090%)\tLoss: 0.007904\n",
      "Train Epoch: 3 [5000/5000 (100%)]\tLoss: 0.007731\t\t\t\n",
      "Test accuracy: 5021/10000 (50.210%)\tLoss: 0.007359\n",
      "Train Epoch: 4 [5000/5000 (100%)]\tLoss: 0.005088\t\t\t\n",
      "Test accuracy: 5083/10000 (50.830%)\tLoss: 0.009341\n",
      "Train Epoch: 5 [5000/5000 (100%)]\tLoss: 0.005476\t\t\t\n",
      "Test accuracy: 5023/10000 (50.230%)\tLoss: 0.004196\n",
      "Train Epoch: 6 [5000/5000 (100%)]\tLoss: 0.004383\t\t\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-88fe3f556335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-b46fc2c08fd0>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(embedding_net, model, device, test_loader, loss_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-adca79e68b61>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimg_ar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2770\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2772\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet().to(device)\n",
    "model = Net(embedding_net).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = TripletLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    train(embedding_net, model, device, train_loader, epoch, optimizer,loss_fn)\n",
    "    test(embedding_net, model, device, test_loader,loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two images are of the same number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These two images are of the same number\n"
     ]
    }
   ],
   "source": [
    "prediction_loader = torch.utils.data.DataLoader(MNISTPairs('~/.pytorch/MNIST/', train=False, transform=trans), batch_size=1, shuffle=True)\n",
    "\n",
    "triplets = next(iter(prediction_loader))\n",
    "\n",
    "data = []\n",
    "data.extend(triplets[0][:3:2])\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(data[0].numpy()[0,0])\n",
    "axes[1].imshow(data[1].numpy()[0,0])\n",
    "plt.show()\n",
    "same = oneshot(embedding_net, model, device, data)\n",
    "if same > 0:\n",
    "    print('These two images are of the same number')\n",
    "else:\n",
    "    print('These two images are not of the same number')\n",
    "    \n",
    "data = []\n",
    "data.extend(triplets[0][:2])\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(data[0].numpy()[0,0])\n",
    "axes[1].imshow(data[1].numpy()[0,0])\n",
    "plt.show()\n",
    "same = oneshot(embedding_net, model, device, data)\n",
    "if same > 0:\n",
    "    print('These two images are of the same number')\n",
    "else:\n",
    "    print('These two images are not of the same number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
