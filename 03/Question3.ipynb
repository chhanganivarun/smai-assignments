{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset= datasets.MNIST('~/.pytorch/MNIST/', train=True, download=True, transform=transform)\n",
    "\n",
    "validset= datasets.MNIST('~/.pytorch/MNIST/', train=True, download=True, transform=transform)\n",
    "\n",
    "testset= datasets.MNIST('~/.pytorch/MNIST/', train=False, download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "pin_memory = 1\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(trainset)\n",
    "num_test = len(testset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.3 * num_train))\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, sampler=train_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=batch_size, sampler=valid_sampler,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, num_workers=num_workers, \n",
    "    pin_memory=pin_memory, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for imgs,labels in train_loader:\n",
    "    print(imgs.shape,labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, hidden_channels=128):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(784,500)\n",
    "        self.dense2 = nn.Linear(500,100)\n",
    "        self.dense3 = nn.Linear(100,100)\n",
    "        self.dense4 = nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view(x.shape[0],-1)\n",
    "        out = F.relu(self.dense1(out))\n",
    "        out = F.relu(self.dense2(out))\n",
    "        out = F.relu(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for imgs,labels in valid_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "        total += imgs.shape[0]\n",
    "\n",
    "        correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "    accuracy = correct/total\n",
    "    return accuracy.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XavierUniformInit(model):\n",
    "    if 'module' in model._modules.keys():\n",
    "        # in case of DataParallel\n",
    "        for key,val in model._modules['module']._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.xavier_uniform_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))\n",
    "    else:\n",
    "        for key,val in model._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.xavier_uniform_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UniformInit(model):\n",
    "    if 'module' in model._modules.keys():\n",
    "        # in case of DataParallel\n",
    "        for key,val in model._modules['module']._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.uniform_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))\n",
    "    else:\n",
    "        for key,val in model._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.uniform_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalInit(model):\n",
    "    if 'module' in model._modules.keys():\n",
    "        # in case of DataParallel\n",
    "        for key,val in model._modules['module']._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.normal_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))\n",
    "    else:\n",
    "        for key,val in model._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.normal_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroInit(model):\n",
    "    if 'module' in model._modules.keys():\n",
    "        # in case of DataParallel\n",
    "        for key,val in model._modules['module']._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.normal_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))\n",
    "    else:\n",
    "        for key,val in model._modules.items():\n",
    "            if hasattr(val,'weight'):\n",
    "                try:\n",
    "                    nn.init.zeros_(val.weight.data)\n",
    "                    nn.init.zeros_(val.bias.data)\n",
    "                except:\n",
    "                    print('Couldn\\'t initialize for {}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefaultInit(model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.040975. Test Loss: 0.060895. Test Accuracy: 95.00\n",
      "Initialization: DefaultInit .Final Validation Accuracy: 97.28888702392578\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.007937. Test Loss: 0.025502. Test Accuracy: 97.000\n",
      "Initialization: XavierUniformInit .Final Validation Accuracy: 97.55000305175781\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 89.795143. Test Loss: 68.250000. Test Accuracy: 73.000000.00\n",
      "Initialization: UniformInit .Final Validation Accuracy: 67.22777557373047\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 80.914796. Test Loss: 132.954391. Test Accuracy: 84.0000\n",
      "Initialization: NormalInit .Final Validation Accuracy: 88.69999694824219\n"
     ]
    }
   ],
   "source": [
    "for func in [DefaultInit,XavierUniformInit,UniformInit,NormalInit]:\n",
    "    model = MLPClassifier().to(device)\n",
    "    func(model)\n",
    "\n",
    "    lr = 1e-4\n",
    "    optimizer = optim.Adam(model.parameters(),lr= lr,weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = 25\n",
    "    iter_n = 0\n",
    "    for e in range(epochs):\n",
    "        iter_n = 0\n",
    "        cumu_loss = 0\n",
    "    #     print(\"Epoch %d of %d\"%(e+1,epochs))\n",
    "        for imgs,labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    #         print(imgs.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumu_loss += loss.item()\n",
    "\n",
    "            iter_n+=1\n",
    "    #         print(iter_n)\n",
    "            if iter_n % 420 == 0:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "\n",
    "                for imgs,labels in valid_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    preds = model(imgs)\n",
    "                    outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "                    total += imgs.shape[0]\n",
    "\n",
    "                    correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "\n",
    "                    if total>= 50:\n",
    "                        break\n",
    "                accuracy = correct/total\n",
    "                print('\\rEpoch {} of {}. Iteration: {}. Train Loss: {:.6f}. Test Loss: {:.6f}. Test Accuracy: {:.2f}'.format(e+1, epochs, iter_n, cumu_loss/iter_n, loss.item(), accuracy), end='')\n",
    "        scheduler.step()\n",
    "\n",
    "    print('\\nInitialization: {} .Final Validation Accuracy: {}'.format(func.__name__,test(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.036079. Test Loss: 0.037493. Test Accuracy: 95.000\n",
      "Initialization: DefaultInit .Final Validation Accuracy: 97.05555725097656\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.007074. Test Loss: 0.038554. Test Accuracy: 95.00\n",
      "Initialization: XavierUniformInit .Final Validation Accuracy: 97.62777709960938\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 1960.465335. Test Loss: 1855.805054. Test Accuracy: 30.0000\n",
      "Initialization: UniformInit .Final Validation Accuracy: 38.36666488647461\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 64.181565. Test Loss: 108.768669. Test Accuracy: 94.0000\n",
      "Initialization: NormalInit .Final Validation Accuracy: 88.77222442626953\n"
     ]
    }
   ],
   "source": [
    "for func in [DefaultInit,XavierUniformInit,UniformInit,NormalInit]:\n",
    "    model = MLPClassifier().to(device)\n",
    "    func(model)\n",
    "\n",
    "    lr = 1e-4\n",
    "    optimizer = optim.RMSprop(model.parameters(),lr= lr,weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = 25\n",
    "    iter_n = 0\n",
    "    for e in range(epochs):\n",
    "        iter_n = 0\n",
    "        cumu_loss = 0\n",
    "    #     print(\"Epoch %d of %d\"%(e+1,epochs))\n",
    "        for imgs,labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    #         print(imgs.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumu_loss += loss.item()\n",
    "\n",
    "            iter_n+=1\n",
    "    #         print(iter_n)\n",
    "            if iter_n % 420 == 0:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "\n",
    "                for imgs,labels in valid_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    preds = model(imgs)\n",
    "                    outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "                    total += imgs.shape[0]\n",
    "\n",
    "                    correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "\n",
    "                    if total>= 50:\n",
    "                        break\n",
    "                accuracy = correct/total\n",
    "                print('\\rEpoch {} of {}. Iteration: {}. Train Loss: {:.6f}. Test Loss: {:.6f}. Test Accuracy: {:.2f}'.format(e+1, epochs, iter_n, cumu_loss/iter_n, loss.item(), accuracy), end='')\n",
    "        scheduler.step()\n",
    "\n",
    "    print('\\nInitialization: {} .Final Validation Accuracy: {}'.format(func.__name__,test(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.000137. Test Loss: 0.000064. Test Accuracy: 98.000\n",
      "Initialization: DefaultInit .Final Validation Accuracy: 98.05555725097656\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.000131. Test Loss: 0.000069. Test Accuracy: 97.00\n",
      "Initialization: XavierUniformInit .Final Validation Accuracy: 98.15555572509766\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 2.301801. Test Loss: 2.305720. Test Accuracy: 5.0000\n",
      "Initialization: UniformInit .Final Validation Accuracy: 11.077777862548828\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: nan. Test Loss: nan. Test Accuracy: 11.00\n",
      "Initialization: NormalInit .Final Validation Accuracy: 10.027777671813965\n"
     ]
    }
   ],
   "source": [
    "for func in [DefaultInit,XavierUniformInit,UniformInit,NormalInit]:\n",
    "    model = MLPClassifier().to(device)\n",
    "    func(model)\n",
    "\n",
    "    lr = 0.1\n",
    "    optimizer = optim.SGD(model.parameters(),lr= lr,weight_decay=1e-5,momentum=0.7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = 25\n",
    "    iter_n = 0\n",
    "    for e in range(epochs):\n",
    "        iter_n = 0\n",
    "        cumu_loss = 0\n",
    "    #     print(\"Epoch %d of %d\"%(e+1,epochs))\n",
    "        for imgs,labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    #         print(imgs.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumu_loss += loss.item()\n",
    "\n",
    "            iter_n+=1\n",
    "    #         print(iter_n)\n",
    "            if iter_n % 420 == 0:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "\n",
    "                for imgs,labels in valid_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    preds = model(imgs)\n",
    "                    outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "                    total += imgs.shape[0]\n",
    "\n",
    "                    correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "\n",
    "                    if total>= 50:\n",
    "                        break\n",
    "                accuracy = correct/total\n",
    "                print('\\rEpoch {} of {}. Iteration: {}. Train Loss: {:.6f}. Test Loss: {:.6f}. Test Accuracy: {:.2f}'.format(e+1, epochs, iter_n, cumu_loss/iter_n, loss.item(), accuracy), end='')\n",
    "        scheduler.step()\n",
    "\n",
    "    print('\\nInitialization: {} .Final Validation Accuracy: {}'.format(func.__name__,test(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD without momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.000334. Test Loss: 0.000226. Test Accuracy: 96.000\n",
      "Initialization: DefaultInit .Final Validation Accuracy: 98.08333587646484\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.000346. Test Loss: 0.000151. Test Accuracy: 96.00\n",
      "Initialization: XavierUniformInit .Final Validation Accuracy: 98.03333282470703\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: 2.301634. Test Loss: 2.306773. Test Accuracy: 10.0000\n",
      "Initialization: UniformInit .Final Validation Accuracy: 11.077777862548828\n",
      "Epoch 25 of 25. Iteration: 420. Train Loss: nan. Test Loss: nan. Test Accuracy: 11.00\n",
      "Initialization: NormalInit .Final Validation Accuracy: 10.027777671813965\n"
     ]
    }
   ],
   "source": [
    "for func in [DefaultInit,XavierUniformInit,UniformInit,NormalInit]:\n",
    "    model = MLPClassifier().to(device)\n",
    "    func(model)\n",
    "\n",
    "    lr = 0.2\n",
    "    optimizer = optim.SGD(model.parameters(),lr= lr,weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "    epochs = 25\n",
    "    iter_n = 0\n",
    "    for e in range(epochs):\n",
    "        iter_n = 0\n",
    "        cumu_loss = 0\n",
    "    #     print(\"Epoch %d of %d\"%(e+1,epochs))\n",
    "        for imgs,labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "    #         print(imgs.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds,labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumu_loss += loss.item()\n",
    "\n",
    "            iter_n+=1\n",
    "    #         print(iter_n)\n",
    "            if iter_n % 420 == 0:\n",
    "                total = 0\n",
    "                correct = 0\n",
    "\n",
    "                for imgs,labels in valid_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    preds = model(imgs)\n",
    "                    outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "                    total += imgs.shape[0]\n",
    "\n",
    "                    correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "\n",
    "                    if total>= 50:\n",
    "                        break\n",
    "                accuracy = correct/total\n",
    "                print('\\rEpoch {} of {}. Iteration: {}. Train Loss: {:.6f}. Test Loss: {:.6f}. Test Accuracy: {:.2f}'.format(e+1, epochs, iter_n, cumu_loss/iter_n, loss.item(), accuracy), end='')\n",
    "        scheduler.step()\n",
    "\n",
    "    print('\\nInitialization: {} .Final Validation Accuracy: {}'.format(func.__name__,test(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels=128):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(1,32,kernel_size=3,padding=1)\n",
    "        self.cnn2 = nn.Conv2d(32,64,kernel_size=3,padding=1)\n",
    "        self.cnn3 = nn.Conv2d(64,hidden_channels,kernel_size=7)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,return_indices=True)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        \n",
    "        self.t_cnn1 = nn.ConvTranspose2d(hidden_channels,64,kernel_size=7)\n",
    "        self.t_cnn2 = nn.ConvTranspose2d(64,32,kernel_size=3,padding=1)\n",
    "        self.t_cnn3 = nn.ConvTranspose2d(32,1,kernel_size=3,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.cnn1(x))\n",
    "        out,ind1 = self.pool(out)\n",
    "        \n",
    "        out = F.relu(self.cnn2(out))\n",
    "        out,ind2 = self.pool(out)\n",
    "\n",
    "        out = self.cnn3(out)\n",
    "        \n",
    "        out = F.relu(self.t_cnn1(out))\n",
    "        out = self.unpool(out,ind2)\n",
    "\n",
    "        out = F.relu(self.t_cnn2(out))\n",
    "        out = self.unpool(out,ind1)\n",
    "\n",
    "        out = torch.sigmoid(self.t_cnn3(out))\n",
    "        \n",
    "        return out\n",
    "    def encoder(self,x):\n",
    "        out = F.relu(self.cnn1(x))\n",
    "        out,ind1 = self.pool(out)\n",
    "        \n",
    "        out = F.relu(self.cnn2(out))\n",
    "        out,ind2 = self.pool(out)\n",
    "\n",
    "        out = self.cnn3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (cnn1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cnn2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cnn3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  (t_cnn1): ConvTranspose2d(64, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (t_cnn2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (t_cnn3): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae = AutoEncoder(hidden_channels=64).to(device)\n",
    "ae.load_state_dict(torch.load('q2.pth'))\n",
    "ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_encoded(nn.Module):\n",
    "    def __init__(self,ae,hidden_channels=64):\n",
    "        super().__init__()\n",
    "        self.ae = ae\n",
    "        ae.eval()\n",
    "        self.dense1 = nn.Linear(hidden_channels,32)\n",
    "        self.dense2 = nn.Linear(32,10)\n",
    "    def forward(self,x):\n",
    "        out = ae.encoder(x)\n",
    "        out = out.view(out.shape[0],-1)\n",
    "        out = F.relu(self.dense1(out))\n",
    "        out = self.dense2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_encoded(ae,hidden_channels=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 25. Iteration: 420. Train Loss: 0.003632. Test Loss: 0.002136. Test Accuracy: 98.000\n",
      "Final Validation Accuracy: 98.8388900756836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.RMSprop(model.parameters(),lr= lr,weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "epochs = 25\n",
    "iter_n = 0\n",
    "for e in range(epochs):\n",
    "    iter_n = 0\n",
    "    cumu_loss = 0\n",
    "#     print(\"Epoch %d of %d\"%(e+1,epochs))\n",
    "    for imgs,labels in train_loader:\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "#         print(imgs.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumu_loss += loss.item()\n",
    "\n",
    "        iter_n+=1\n",
    "#         print(iter_n)\n",
    "        if iter_n % 420 == 0:\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for imgs,labels in valid_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(imgs)\n",
    "                outputs = torch.argmax(preds.data,1)\n",
    "\n",
    "                total += imgs.shape[0]\n",
    "\n",
    "                correct += 100.0*(outputs.cpu() == labels.cpu()).sum()\n",
    "\n",
    "                if total>= 50:\n",
    "                    break\n",
    "            accuracy = correct/total\n",
    "            print('\\rEpoch {} of {}. Iteration: {}. Train Loss: {:.6f}. Test Loss: {:.6f}. Test Accuracy: {:.2f}'.format(e+1, epochs, iter_n, cumu_loss/iter_n, loss.item(), accuracy), end='')\n",
    "    scheduler.step()\n",
    "\n",
    "print('\\nFinal Validation Accuracy: {}'.format(test(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison:\n",
    "Encoded followed by MLP: 98.839%\n",
    "<br>\n",
    "Best MLP(SGD with momentum): 98.155%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
